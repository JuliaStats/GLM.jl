var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Types-defined-in-the-package","page":"API","title":"Types defined in the package","text":"","category":"section"},{"location":"api/#Constructors-for-models","page":"API","title":"Constructors for models","text":"The most general approach to fitting a model is with the fit function, as in\n\njulia> using RDatasets\n\njulia> df = RDatasets.dataset(\"mlmRev\", \"Oxboys\");\n\njulia> fit(LinearModel, hcat(ones(nrow(df)), df.Age), df.Height)\nLinearModel\n\nCoefficients:\n─────────────────────────────────────────────────────────────────\n        Coef.  Std. Error       t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────\nx1  149.372      0.528565  282.60    <1e-99  148.33     150.413\nx2    6.52102    0.816987    7.98    <1e-13    4.91136    8.13068\n─────────────────────────────────────────────────────────────────\n\nThis model can also be fit as\n\njulia> lm(hcat(ones(nrow(df)), df.Age), df.Height)\nLinearModel\n\nCoefficients:\n─────────────────────────────────────────────────────────────────\n        Coef.  Std. Error       t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────\nx1  149.372      0.528565  282.60    <1e-99  148.33     150.413\nx2    6.52102    0.816987    7.98    <1e-13    4.91136    8.13068\n─────────────────────────────────────────────────────────────────","category":"section"},{"location":"api/#Model-methods","page":"API","title":"Model methods","text":"","category":"section"},{"location":"api/#Links-and-methods-applied-to-them","page":"API","title":"Links and methods applied to them","text":"","category":"section"},{"location":"api/#GLM.LinearModel","page":"API","title":"GLM.LinearModel","text":"LinearModel\n\nA combination of a LmResp, a LinPred, and possibly a FormulaTerm\n\nMembers\n\nrr: a LmResp object\npp: a LinPred object\nf: either a FormulaTerm object or nothing\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.DensePredChol","page":"API","title":"GLM.DensePredChol","text":"DensePredChol{T}\n\nA LinPred type with a dense Cholesky factorization of X'X\n\nMembers\n\nX: model matrix of size n × p with n ≥ p.  Should be full column rank.\nbeta0: base coefficient vector of length p\ndelbeta: increment to coefficient vector, also of length p\nscratchbeta: scratch vector of length p, used in linpred! method\nchol: a Cholesky object created from X'X, possibly using row weights.\nscratchm1: scratch Matrix{T} of the same size as X\nscratchm2: scratch Matrix{T} os the same size as X'X\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.DensePredQR","page":"API","title":"GLM.DensePredQR","text":"DensePredQR\n\nA LinPred type with a dense QR decomposition of X\n\nMembers\n\nX: Model matrix of size n × p with n ≥ p.  Should be full column rank.\nbeta0: base coefficient vector of length p\ndelbeta: increment to coefficient vector, also of length p\nscratchbeta: scratch vector of length p, used in linpred! method\nqr: either a QRCompactWY or QRPivoted object created from X, with optional row weights.\nscratchm1: scratch Matrix{T} of the same size as X\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.LmResp","page":"API","title":"GLM.LmResp","text":"LmResp\n\nEncapsulates the response for a linear model\n\nMembers\n\nmu: current value of the mean response vector or fitted value\noffset: optional offset added to the linear predictor to form mu\nwts: optional vector of prior frequency (a.k.a. case) weights for observations\ny: observed response vector\n\nEither or both offset and wts may be of length 0\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.GlmResp","page":"API","title":"GLM.GlmResp","text":"GlmResp\n\nThe response vector and various derived vectors in a generalized linear model.\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.LinPred","page":"API","title":"GLM.LinPred","text":"LinPred\n\nAbstract type representing a linear predictor\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.ModResp","page":"API","title":"GLM.ModResp","text":"ModResp\n\nAbstract type representing a model response vector\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.lm","page":"API","title":"GLM.lm","text":"lm(formula, data;\n   [wts::AbstractVector], dropcollinear::Bool=true, method::Symbol=:qr,\n   contrasts::AbstractDict{Symbol}=Dict{Symbol,Any}())\nlm(X::AbstractMatrix, y::AbstractVector;\n   wts::AbstractVector=similar(y, 0), dropcollinear::Bool=true, method::Symbol=:cholesky)\n\nFit a linear model to data. An alias for fit(LinearModel, X, y; wts=wts, dropcollinear=dropcollinear, method=method)\n\nIn the first method, formula must be a StatsModels.jl Formula object and data a table (in the Tables.jl definition, e.g. a data frame). In the second method, X must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and y must be a vector holding values of the dependent variable.\n\nKeyword Arguments\n\ndropcollinear::Bool: Controls whether or not a model matrix less-than-full rank is accepted. If true (the default) the coefficient for redundant linearly dependent columns is 0.0 and all associated statistics are set to NaN. Typically from a set of linearly-dependent columns the last ones are identified as redundant (however, the exact selection of columns identified as redundant is not guaranteed).\nmethod::Symbol: Controls which decomposition method to use. If method=:qr (the default), then the QR decomposition method will be used. If method=:cholesky, then the Cholesky decomposition method will be used. The Cholesky decomposition is faster and more computationally efficient than QR, but is less numerically stable and thus may fail or produce less accurate estimates for some models.\nwts::Vector: Prior frequency (a.k.a. case) weights of observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software. Can be length 0 to indicate no weighting (default).\ncontrasts::AbstractDict{Symbol}: a Dict mapping term names (as Symbols) to term types (e.g. ContinuousTerm) or contrasts (e.g., HelmertCoding(), SeqDiffCoding(; levels=[\"a\", \"b\", \"c\"]), etc.). If contrasts are not provided for a variable, the appropriate term type will be guessed based on the data type from the data column: any numeric data is assumed to be continuous, and any non-numeric data is assumed to be categorical (with DummyCoding() as the default contrast type).\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.glm","page":"API","title":"GLM.glm","text":"glm(formula, data,\n    distr::UnivariateDistribution, link::Link = canonicallink(distr); <keyword arguments>)\nglm(X::AbstractMatrix, y::AbstractVector,\n    distr::UnivariateDistribution, link::Link = canonicallink(distr); <keyword arguments>)\n\nFit a generalized linear model to data. Alias for fit(GeneralizedLinearModel, ...).\n\nIn the first method, formula must be a StatsModels.jl Formula object and data a table (in the Tables.jl definition, e.g. a data frame). In the second method, X must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and y must be a vector holding values of the dependent variable. In both cases, distr must specify the distribution, and link may specify the link function (if omitted, it is taken to be the canonical link for distr; see Link for a list of built-in links).\n\nKeyword Arguments\n\ndropcollinear::Bool: Controls whether or not a model matrix less-than-full rank is accepted. If true (the default) the coefficient for redundant linearly dependent columns is 0.0 and all associated statistics are set to NaN. Typically from a set of linearly-dependent columns the last ones are identified as redundant (however, the exact selection of columns identified as redundant is not guaranteed).\nmethod::Symbol: Controls which decomposition method to use. If method=:qr (the default), then the QR decomposition method will be used. If method=:cholesky, then the Cholesky decomposition method will be used. The Cholesky decomposition is faster and more computationally efficient than QR, but is less numerically stable and thus may fail or produce less accurate estimates for some models.\nwts::Vector: Prior frequency (a.k.a. case) weights of observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software. Can be length 0 to indicate no weighting (default).\ncontrasts::AbstractDict{Symbol}: a Dict mapping term names (as Symbols) to term types (e.g. ContinuousTerm) or contrasts (e.g., HelmertCoding(), SeqDiffCoding(; levels=[\"a\", \"b\", \"c\"]), etc.). If contrasts are not provided for a variable, the appropriate term type will be guessed based on the data type from the data column: any numeric data is assumed to be continuous, and any non-numeric data is assumed to be categorical (with DummyCoding() as the default contrast type).\noffset::Vector=similar(y,0): offset added to Xβ to form eta.  Can be of length 0\nmaxiter::Integer=30: Maximum number of iterations allowed to achieve convergence\natol::Real=1e-6: Convergence is achieved when the relative change in deviance is less than max(rtol*dev, atol).\nrtol::Real=1e-6: Convergence is achieved when the relative change in deviance is less than max(rtol*dev, atol).\nminstepfac::Real=0.001: Minimum line step fraction. Must be between 0 and 1.\nstart::AbstractVector=nothing: Starting values for beta. Should have the same length as the number of columns in the model matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.negbin","page":"API","title":"GLM.negbin","text":"negbin(formula, data, [link::Link];\n       <keyword arguments>)\nnegbin(X::AbstractMatrix, y::AbstractVector, [link::Link];\n       <keyword arguments>)\n\nFit a negative binomial generalized linear model to data, while simultaneously estimating the shape parameter θ. Extra arguments and keyword arguments will be passed to glm.\n\nIn the first method, formula must be a StatsModels.jl Formula object and data a table (in the Tables.jl definition, e.g. a data frame). In the second method, X must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and y must be a vector holding values of the dependent variable. In both cases, link may specify the link function (if omitted, it is taken to be NegativeBinomial(θ)).\n\nKeyword Arguments\n\ninitialθ::Real=Inf: Starting value for shape parameter θ. If it is Inf then the initial value will be estimated by fitting a Poisson distribution.\ndropcollinear::Bool=true: See dropcollinear for glm\nmethod::Symbol=:qr: See method for glm\nmaxiter::Integer=30: See maxiter for glm\natol::Real=1.0e-6: See atol for glm\nrtol::Real=1.0e-6: See rtol for glm\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsAPI.fit","page":"API","title":"StatsAPI.fit","text":"fit(LinearModel, formula::FormulaTerm, data;\n    [wts::AbstractVector], dropcollinear::Bool=true, method::Symbol=:qr,\n    contrasts::AbstractDict{Symbol}=Dict{Symbol,Any}())\nfit(LinearModel, X::AbstractMatrix, y::AbstractVector;\n    wts::AbstractVector=similar(y, 0), dropcollinear::Bool=true, method::Symbol=:qr)\n\nFit a linear model to data.\n\nIn the first method, formula must be a StatsModels.jl Formula object and data a table (in the Tables.jl definition, e.g. a data frame). In the second method, X must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and y must be a vector holding values of the dependent variable.\n\nKeyword Arguments\n\ndropcollinear::Bool: Controls whether or not a model matrix less-than-full rank is accepted. If true (the default) the coefficient for redundant linearly dependent columns is 0.0 and all associated statistics are set to NaN. Typically from a set of linearly-dependent columns the last ones are identified as redundant (however, the exact selection of columns identified as redundant is not guaranteed).\nmethod::Symbol: Controls which decomposition method to use. If method=:qr (the default), then the QR decomposition method will be used. If method=:cholesky, then the Cholesky decomposition method will be used. The Cholesky decomposition is faster and more computationally efficient than QR, but is less numerically stable and thus may fail or produce less accurate estimates for some models.\nwts::Vector: Prior frequency (a.k.a. case) weights of observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software. Can be length 0 to indicate no weighting (default).\ncontrasts::AbstractDict{Symbol}: a Dict mapping term names (as Symbols) to term types (e.g. ContinuousTerm) or contrasts (e.g., HelmertCoding(), SeqDiffCoding(; levels=[\"a\", \"b\", \"c\"]), etc.). If contrasts are not provided for a variable, the appropriate term type will be guessed based on the data type from the data column: any numeric data is assumed to be continuous, and any non-numeric data is assumed to be categorical (with DummyCoding() as the default contrast type).\n\n\n\n\n\nfit(GeneralizedLinearModel, formula, data,\n    distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\nfit(GeneralizedLinearModel, X::AbstractMatrix, y::AbstractVector,\n    distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\n\nFit a generalized linear model to data.\n\nIn the first method, formula must be a StatsModels.jl Formula object and data a table (in the Tables.jl definition, e.g. a data frame). In the second method, X must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and y must be a vector holding values of the dependent variable. In both cases, distr must specify the distribution, and link may specify the link function (if omitted, it is taken to be the canonical link for distr; see Link for a list of built-in links).\n\nKeyword Arguments\n\ndropcollinear::Bool: Controls whether or not a model matrix less-than-full rank is accepted. If true (the default) the coefficient for redundant linearly dependent columns is 0.0 and all associated statistics are set to NaN. Typically from a set of linearly-dependent columns the last ones are identified as redundant (however, the exact selection of columns identified as redundant is not guaranteed).\nmethod::Symbol: Controls which decomposition method to use. If method=:qr (the default), then the QR decomposition method will be used. If method=:cholesky, then the Cholesky decomposition method will be used. The Cholesky decomposition is faster and more computationally efficient than QR, but is less numerically stable and thus may fail or produce less accurate estimates for some models.\nwts::Vector: Prior frequency (a.k.a. case) weights of observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software. Can be length 0 to indicate no weighting (default).\ncontrasts::AbstractDict{Symbol}: a Dict mapping term names (as Symbols) to term types (e.g. ContinuousTerm) or contrasts (e.g., HelmertCoding(), SeqDiffCoding(; levels=[\"a\", \"b\", \"c\"]), etc.). If contrasts are not provided for a variable, the appropriate term type will be guessed based on the data type from the data column: any numeric data is assumed to be continuous, and any non-numeric data is assumed to be categorical (with DummyCoding() as the default contrast type).\noffset::Vector=similar(y,0): offset added to Xβ to form eta.  Can be of length 0\nmaxiter::Integer=30: Maximum number of iterations allowed to achieve convergence\natol::Real=1e-6: Convergence is achieved when the relative change in deviance is less than max(rtol*dev, atol).\nrtol::Real=1e-6: Convergence is achieved when the relative change in deviance is less than max(rtol*dev, atol).\nminstepfac::Real=0.001: Minimum line step fraction. Must be between 0 and 1.\nstart::AbstractVector=nothing: Starting values for beta. Should have the same length as the number of columns in the model matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsAPI.cooksdistance","page":"API","title":"StatsAPI.cooksdistance","text":"cooksdistance(obj::LinearModel)\n\nCompute Cook's distance for each observation in linear model obj, giving an estimate of the influence of each data point. Currently only implemented for linear models without weights.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsAPI.deviance","page":"API","title":"StatsAPI.deviance","text":"deviance(obj::LinearModel)\n\nFor linear models, the deviance is equal to the residual sum of squares (RSS).\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.dispersion","page":"API","title":"GLM.dispersion","text":"dispersion(m::AbstractGLM, sqr::Bool=false)\n\nReturn the estimated dispersion (or scale) parameter for a model's distribution, generally written σ for linear models and ϕ for generalized linear models. It is, by definition, equal to 1 for the Bernoulli, Binomial, and Poisson families.\n\nIf sqr is true, the squared dispersion parameter is returned.\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.ftest","page":"API","title":"GLM.ftest","text":"ftest(mod::LinearModel)\n\nPerform an F-test to determine whether model mod fits significantly better than the null model (i.e. which includes only the intercept).\n\njulia> dat = DataFrame(Result=[1.1, 1.2, 1, 2.2, 1.9, 2, 0.9, 1, 1, 2.2, 2, 2],\n                       Treatment=[1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2]);\n\n\njulia> model = lm(@formula(Result ~ 1 + Treatment), dat);\n\n\njulia> ftest(model)\nF-test against the null model:\nF-statistic: 241.62 on 12 observations and 1 degrees of freedom, p-value: <1e-07\n\n\n\n\n\nftest(mod::LinearModel...; atol::Real=0.0)\n\nFor each sequential pair of linear models in mod..., perform an F-test to determine if the one model fits significantly better than the other. Models must have been fitted on the same data, and be nested either in forward or backward direction.\n\nA table is returned containing consumed degrees of freedom (DOF), difference in DOF from the preceding model, sum of squared residuals (SSR), difference in SSR from the preceding model, R², difference in R² from the preceding model, and F-statistic and p-value for the comparison between the two models.\n\nnote: Note\nThis function can be used to perform an ANOVA by testing the relative fit of two models to the data\n\nOptional keyword argument atol controls the numerical tolerance when testing whether the models are nested.\n\nExamples\n\nSuppose we want to compare the effects of two or more treatments on some result. Because this is an ANOVA, our null hypothesis is that Result ~ 1 fits the data as well as Result ~ 1 + Treatment.\n\njulia> dat = DataFrame(Result=[1.1, 1.2, 1, 2.2, 1.9, 2, 0.9, 1, 1, 2.2, 2, 2],\n                       Treatment=[1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2],\n                       Other=categorical([1, 1, 2, 1, 2, 1, 3, 1, 1, 2, 2, 1]));\n\n\njulia> nullmodel = lm(@formula(Result ~ 1), dat);\n\n\njulia> model = lm(@formula(Result ~ 1 + Treatment), dat);\n\n\njulia> bigmodel = lm(@formula(Result ~ 1 + Treatment + Other), dat);\n\njulia> ftest(nullmodel, model)\nF-test: 2 models fitted on 12 observations\n─────────────────────────────────────────────────────────────────\n     DOF  ΔDOF     SSR     ΔSSR      R²     ΔR²        F*   p(>F)\n─────────────────────────────────────────────────────────────────\n[1]    2        3.2292           0.0000\n[2]    3     1  0.1283  -3.1008  0.9603  0.9603  241.6234  <1e-07\n─────────────────────────────────────────────────────────────────\n\njulia> ftest(nullmodel, model, bigmodel)\nF-test: 3 models fitted on 12 observations\n─────────────────────────────────────────────────────────────────\n     DOF  ΔDOF     SSR     ΔSSR      R²     ΔR²        F*   p(>F)\n─────────────────────────────────────────────────────────────────\n[1]    2        3.2292           0.0000\n[2]    3     1  0.1283  -3.1008  0.9603  0.9603  241.6234  <1e-07\n[3]    5     2  0.1017  -0.0266  0.9685  0.0082    1.0456  0.3950\n─────────────────────────────────────────────────────────────────\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsAPI.nobs","page":"API","title":"StatsAPI.nobs","text":"nobs(obj::LinearModel)\nnobs(obj::GLM)\n\nFor linear and generalized linear models, returns the number of rows, or, when prior weights are specified, the sum of weights.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsAPI.nulldeviance","page":"API","title":"StatsAPI.nulldeviance","text":"nulldeviance(obj::LinearModel)\n\nFor linear models, the deviance of the null model is equal to the total sum of squares (TSS).\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsAPI.predict","page":"API","title":"StatsAPI.predict","text":"predict(mm::LinearModel, newx::AbstractMatrix;\n        interval::Union{Symbol,Nothing} = nothing, level::Real = 0.95)\n\nIf interval is nothing (the default), return a vector with the predicted values for model mm and new data newx. Otherwise, return a vector with the predicted values, as well as vectors with the lower and upper confidence bounds for a given level (0.95 equates alpha = 0.05). Valid values of interval are :confidence delimiting the  uncertainty of the predicted relationship, and :prediction delimiting estimated bounds for new data points.\n\n\n\n\n\npredict(mm::AbstractGLM, newX;\n        offset::FPVector=[],\n        interval::Union{Symbol,Nothing}=nothing, level::Real=0.95,\n        interval_method::Symbol=:transformation)\n\nReturn the predicted response of model mm from covariate values newX and, optionally, an offset.\n\nnewX must be either a table (in the Tables.jl definition) containing all columns used in the model formula, or a matrix with one column for each predictor in the model. In both cases, each row represents an observation for which a prediction will be returned.\n\nIf interval=:confidence, also return upper and lower bounds for a given coverage level. By default (interval_method = :transformation) the intervals are constructed by applying the inverse link to intervals for the linear predictor. If interval_method = :delta, the intervals are constructed by the delta method, i.e., by linearization of the predicted response around the linear predictor. The :delta method intervals are symmetric around the point estimates, but do not respect natural parameter constraints (e.g., the lower bound for a probability could be negative).\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.Link","page":"API","title":"GLM.Link","text":"Link\n\nAn abstract type whose subtypes refer to link functions.\n\nGLM currently supports the following links: CauchitLink, CloglogLink, IdentityLink, InverseLink, InverseSquareLink, LogitLink, LogLink, NegativeBinomialLink, PowerLink, ProbitLink, SqrtLink.\n\nSubtypes of Link are required to implement methods for GLM.linkfun, GLM.linkinv and GLM.inverselink.\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.Link01","page":"API","title":"GLM.Link01","text":"Link01\n\nAn abstract subtype of Link which are links defined on (0, 1)\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.CauchitLink","page":"API","title":"GLM.CauchitLink","text":"CauchitLink\n\nA Link01 corresponding to the standard Cauchy distribution, Distributions.Cauchy.\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.CloglogLink","page":"API","title":"GLM.CloglogLink","text":"CloglogLink\n\nA Link01 corresponding to the extreme value (or log-Weibull) distribution.  The link is the complementary log-log transformation, log(1 - log(-μ)).\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.IdentityLink","page":"API","title":"GLM.IdentityLink","text":"IdentityLink\n\nThe canonical Link for the Normal distribution, defined as η = μ.\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.InverseLink","page":"API","title":"GLM.InverseLink","text":"InverseLink\n\nThe canonical Link for Distributions.Gamma distribution, defined as η = inv(μ).\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.InverseSquareLink","page":"API","title":"GLM.InverseSquareLink","text":"InverseSquareLink\n\nThe canonical Link for Distributions.InverseGaussian distribution, defined as η = inv(abs2(μ)).\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.LogitLink","page":"API","title":"GLM.LogitLink","text":"LogitLink\n\nThe canonical Link01 for Distributions.Bernoulli and Distributions.Binomial. The inverse link, linkinv, is the c.d.f. of the standard logistic distribution, Distributions.Logistic.\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.LogLink","page":"API","title":"GLM.LogLink","text":"LogLink\n\nThe canonical Link for Distributions.Poisson, defined as η = log(μ).\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.NegativeBinomialLink","page":"API","title":"GLM.NegativeBinomialLink","text":"NegativeBinomialLink\n\nThe canonical Link for Distributions.NegativeBinomial distribution, defined as η = log(μ/(μ+θ)). The shape parameter θ has to be fixed for the distribution to belong to the exponential family.\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.PowerLink","page":"API","title":"GLM.PowerLink","text":"PowerLink\n\nA Link defined as η = μ^λ when λ ≠ 0, and to η = log(μ) when λ = 0, i.e. the class of transforms that use a power function or logarithmic function.\n\nMany other links are special cases of PowerLink:\n\nIdentityLink when λ = 1.\nSqrtLink when λ = 0.5.\nLogLink when λ = 0.\nInverseLink when λ = -1.\nInverseSquareLink when λ = -2.\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.ProbitLink","page":"API","title":"GLM.ProbitLink","text":"ProbitLink\n\nA Link01 whose linkinv is the c.d.f. of the standard normal distribution, Distributions.Normal().\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.SqrtLink","page":"API","title":"GLM.SqrtLink","text":"SqrtLink\n\nA Link defined as η = √μ\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.linkfun","page":"API","title":"GLM.linkfun","text":"GLM.linkfun(L::Link, μ::Real)\n\nReturn η, the value of the linear predictor for link L at mean μ.\n\nExamples\n\njulia> μ = inv(10):inv(5):1\n0.1:0.2:0.9\n\njulia> show(linkfun.(LogitLink(), μ))\n[-2.197224577336219, -0.8472978603872036, 0.0, 0.8472978603872034, 2.1972245773362196]\n\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.linkinv","page":"API","title":"GLM.linkinv","text":"GLM.linkinv(L::Link, η::Real)\n\nReturn μ, the mean value, for link L at linear predictor value η.\n\nExamples\n\njulia> μ = 0.1:0.2:1\n0.1:0.2:0.9\n\njulia> η = logit.(μ);\n\n\njulia> linkinv.(LogitLink(), η) ≈ μ\ntrue\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.inverselink","page":"API","title":"GLM.inverselink","text":"GLM.inverselink(L::Link, η::Real)\n\nReturn a 3-tuple of:\n\nthe inverse link,\nthe derivative of the inverse link, and\n(1 - μ) to construct the variance function when typeof(L) <: Link01, or NaN otherwise\n\nExamples\n\njulia> GLM.inverselink(LogitLink(), 0.0)\n(0.5, 0.25, 0.5)\n\njulia> μ, deriv, oneminusμ = GLM.inverselink(CloglogLink(), 0.0);\n\n\n\njulia> μ + oneminusμ ≈ 1\ntrue\n\njulia> μ*(1 - μ) ≈ deriv\nfalse\n\njulia> isnan(last(GLM.inverselink(LogLink(), 2.0)))\ntrue\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.canonicallink","page":"API","title":"GLM.canonicallink","text":"canonicallink(D::Distribution)\n\nReturn the canonical link for distribution D, which must be in the exponential family.\n\nExamples\n\njulia> canonicallink(Bernoulli())\nLogitLink()\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.glmvar","page":"API","title":"GLM.glmvar","text":"GLM.glmvar(D::Distribution, μ::Real)\n\nReturn the value of the variance function for D at μ\n\nThe variance of D at μ is the product of the dispersion parameter, ϕ, which does not depend on μ and the value of glmvar.  In other words glmvar returns the factor of the variance that depends on μ.\n\nExamples\n\njulia> μ = 1/6:1/3:1;\n\njulia> glmvar.(Normal(), μ)    # constant for Normal()\n3-element Vector{Float64}:\n 1.0\n 1.0\n 1.0\n\njulia> glmvar.(Bernoulli(), μ) ≈ μ .* (1 .- μ)\ntrue\n\njulia> glmvar.(Poisson(), μ) == μ\ntrue\n\njulia> glmvar.(Geometric(), μ) ≈ μ .* (1 .+ μ)\ntrue\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.mustart","page":"API","title":"GLM.mustart","text":"GLM.mustart(D::Distribution, y, wt)\n\nReturn a starting value for μ.\n\nFor some distributions it is appropriate to set μ = y to initialize the IRLS algorithm but for others, notably the Bernoulli, the values of y are not allowed as values of μ and must be modified.\n\nExamples\n\njulia> GLM.mustart(Bernoulli(), 0.0, 1) ≈ 1/4\ntrue\n\njulia> GLM.mustart(Bernoulli(), 1.0, 1) ≈ 3/4\ntrue\n\njulia> GLM.mustart(Binomial(), 0.0, 10) ≈ 1/22\ntrue\n\njulia> GLM.mustart(Normal(), 0.0, 1) ≈ 0\ntrue\n\njulia> GLM.mustart(Geometric(), 4, 1) ≈ 4\ntrue\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.devresid","page":"API","title":"GLM.devresid","text":"devresid(D, y, μ::Real)\n\nReturn the squared deviance residual of μ from y for distribution D\n\nThe deviance of a GLM can be evaluated as the sum of the squared deviance residuals.  This is the principal use for these values.  The actual deviance residual, say for plotting, is the signed square root of this value\n\nsign(y - μ) * sqrt(devresid(D, y, μ))\n\nExamples\n\njulia> devresid(Normal(), 0, 0.25) ≈ abs2(0.25)\ntrue\n\njulia> devresid(Bernoulli(), 1, 0.75) ≈ -2*log(0.75)\ntrue\n\njulia> devresid(Bernoulli(), 0, 0.25) ≈ -2*log1p(-0.25)\ntrue\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.dispersion_parameter","page":"API","title":"GLM.dispersion_parameter","text":"GLM.dispersion_parameter(D)\n\nDoes distribution D have a separate dispersion parameter, ϕ?\n\nReturns false for the Bernoulli, Binomial and Poisson distributions, true otherwise.\n\nExamples\n\njulia> show(GLM.dispersion_parameter(Normal()))\ntrue\njulia> show(GLM.dispersion_parameter(Bernoulli()))\nfalse\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.loglik_obs","page":"API","title":"GLM.loglik_obs","text":"GLM.loglik_obs(D, y, μ, wt, ϕ)\n\nReturns wt * logpdf(D(μ, ϕ), y) where the parameters of D are derived from μ and ϕ.\n\nThe wt argument is a multiplier of the result except in the case of the Binomial where wt is the number of trials and μ is the proportion of successes.\n\nThe loglikelihood of a fitted model is the sum of these values over all the observations.\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.cancancel","page":"API","title":"GLM.cancancel","text":"cancancel(r::GlmResp{V,D,L})\n\nReturns true if dμ/dη for link L is the variance function for distribution D\n\nWhen L is the canonical link for D the derivative of the inverse link is a multiple of the variance function for D.  If they are the same a numerator and denominator term in the expression for the working weights will cancel.\n\n\n\n\n\n","category":"function"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Linear-regression","page":"Examples","title":"Linear regression","text":"julia> using DataFrames, GLM, StatsBase\n\njulia> data = DataFrame(X=[1,2,3], Y=[2,4,7])\n3×2 DataFrame\n Row │ X      Y\n     │ Int64  Int64\n─────┼──────────────\n   1 │     1      2\n   2 │     2      4\n   3 │     3      7\n\njulia> ols = lm(@formula(Y ~ X), data)\nLinearModel\n\nY ~ 1 + X\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  -0.666667    0.62361   -1.07    0.4788   -8.59038    7.25704\nX             2.5         0.288675   8.66    0.0732   -1.16797    6.16797\n─────────────────────────────────────────────────────────────────────────\n\njulia> round.(stderror(ols), digits=5)\n2-element Vector{Float64}:\n 0.62361\n 0.28868\n\njulia> round.(predict(ols), digits=5)\n3-element Vector{Float64}:\n 1.83333\n 4.33333\n 6.83333\n\njulia> round.(confint(ols); digits=5)\n2×2 Matrix{Float64}:\n -8.59038  7.25704\n -1.16797  6.16797\n\njulia> round(r2(ols); digits=5)\n0.98684\n\njulia> round(adjr2(ols); digits=5)\n0.97368\n\njulia> round(deviance(ols); digits=5)\n0.16667\n\njulia> dof(ols)\n3\n\njulia> dof_residual(ols)\n1.0\n\njulia> round(aic(ols); digits=5)\n5.84252\n\njulia> round(aicc(ols); digits=5)\n-18.15748\n\njulia> round(bic(ols); digits=5)\n3.13835\n\njulia> round(dispersion(ols); digits=5)\n0.40825\n\njulia> round(loglikelihood(ols); digits=5)\n0.07874\n\njulia> round(nullloglikelihood(ols); digits=5)\n-6.41736\n\njulia> round.(vcov(ols); digits=5)\n2×2 Matrix{Float64}:\n  0.38889  -0.16667\n -0.16667   0.08333\n\nBy default, the lm method uses the Cholesky factorization which is known as fast but numerically unstable, especially for ill-conditioned design matrices. Also, the Cholesky method aggressively detects multicollinearity. You can use the method keyword argument to apply a more stable QR factorization method.\n\njulia> data = DataFrame(X=[1,2,3], Y=[2,4,7]);\n\njulia> ols = lm(@formula(Y ~ X), data; method=:qr)\nLinearModel\n\nY ~ 1 + X\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  -0.666667    0.62361   -1.07    0.4788   -8.59038    7.25704\nX             2.5         0.288675   8.66    0.0732   -1.16797    6.16797\n─────────────────────────────────────────────────────────────────────────\n\nThe following example shows that QR decomposition works better for an ill-conditioned design matrix. The linear model with the QR method is a better model than the linear model with Cholesky decomposition method since the estimated loglikelihood of previous model is higher. Note that, the condition number of the design matrix is quite high (≈ 3.52e7).\n\njulia> X = [-0.4011512997627107 0.6368622664511552;\n            -0.0808472925693535 0.12835204623364604;\n            -0.16931095045225217 0.2687956795496601;\n            -0.4110745650568839 0.6526163576003452;\n            -0.4035951747670475 0.6407421349445884;\n            -0.4649907741370211 0.7382129928076485;\n            -0.15772708898883683 0.25040532268222715;\n            -0.38144358562952446 0.6055745630707645;\n            -0.1012787681395544 0.16078875117643368;\n            -0.2741403589052255 0.4352214984054432];\n\njulia> y = [4.362866166172215,\n            0.8792840060172619,\n            1.8414020451091684,\n            4.470790758717395,\n            4.3894454833815395,\n            5.0571760643993455,\n            1.7154177874916376,\n            4.148527704012107,\n            1.1014936742570425,\n            2.9815131910316097];\n\njulia> modelqr = lm(X, y; method=:qr)\nLinearModel\n\nCoefficients:\n────────────────────────────────────────────────────────────────\n       Coef.  Std. Error       t  Pr(>|t|)  Lower 95%  Upper 95%\n────────────────────────────────────────────────────────────────\nx1   5.00389   0.0560164   89.33    <1e-12    4.87472    5.13307\nx2  10.0025    0.035284   283.48    <1e-16    9.92109   10.0838\n────────────────────────────────────────────────────────────────\n\njulia> modelchol = lm(X, y; method=:cholesky)\nLinearModel\n\nCoefficients:\n────────────────────────────────────────────────────────────────\n       Coef.  Std. Error       t  Pr(>|t|)  Lower 95%  Upper 95%\n────────────────────────────────────────────────────────────────\nx1   5.17647   0.0849184   60.96    <1e-11    4.98065    5.37229\nx2  10.1112    0.053489   189.03    <1e-15    9.98781   10.2345\n────────────────────────────────────────────────────────────────\n\njulia> loglikelihood(modelqr) > loglikelihood(modelchol)\ntrue\n\nSince the Cholesky method with dropcollinear = true aggressively detects multicollinearity, if you ever encounter multicollinearity in any GLM model with Cholesky, it is worth trying the same model with QR decomposition. The following example is taken from Introductory Econometrics: A Modern Approach, 7e\" by Jeffrey M. Wooldridge. The dataset is used to study the relationship between firm size—often measured by annual sales—and spending on research and development (R&D). The following shows that for the given model, the Cholesky method detects multicollinearity in the design matrix with dropcollinear=true and hence does not estimate all parameters as opposed to QR.\n\njulia> y = [9.42190647125244, 2.084805727005, 3.9376676082611, 2.61976027488708, 4.04761934280395, 2.15384602546691,\n            2.66240668296813, 4.39475727081298, 5.74520826339721, 3.59616208076477, 1.54265284538269, 2.59368276596069,\n            1.80476510524749, 1.69270837306976, 3.04201245307922, 2.18389105796813, 2.73844122886657, 2.88134002685546,\n            2.46666669845581, 3.80616021156311, 5.12149810791015, 6.80378007888793, 3.73669862747192, 1.21332454681396,\n            2.54629635810852, 5.1612901687622, 1.86798071861267, 1.21465551853179, 6.31019830703735, 1.02669405937194, \n            2.50623273849487, 1.5936255455017];\n\njulia> x = [4570.2001953125, 2830, 596.799987792968, 133.600006103515, 42, 390, 93.9000015258789, 907.900024414062,\n            19773, 39709, 2936.5, 2513.80004882812, 1124.80004882812, 921.599975585937, 2432.60009765625, 6754,\n            1066.30004882812, 3199.89990234375, 150, 509.700012207031, 1452.69995117187, 8995, 1212.30004882812,\n            906.599975585937, 2592, 201.5, 2617.80004882812, 502.200012207031, 2824, 292.200012207031, 7621, 1631.5];\n\njulia> rdchem = DataFrame(rdintens=y, sales=x);\n\njulia> mdl = lm(@formula(rdintens ~ sales + sales^2), rdchem; method=:cholesky)\nLinearModel\n\nrdintens ~ 1 + sales + :(sales ^ 2)\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────────────────\n                    Coef.     Std. Error       t  Pr(>|t|)      Lower 95%     Upper 95%\n───────────────────────────────────────────────────────────────────────────────────────\n(Intercept)   0.0          NaN            NaN       NaN     NaN            NaN\nsales         0.000852509    0.000156784    5.44    <1e-05    0.000532313    0.00117271\nsales ^ 2    -1.97385e-8     4.56287e-9    -4.33    0.0002   -2.90571e-8    -1.04199e-8\n───────────────────────────────────────────────────────────────────────────────────────\n\njulia> mdl = lm(@formula(rdintens ~ sales + sales^2), rdchem; method=:qr)\nLinearModel\n\nrdintens ~ 1 + sales + :(sales ^ 2)\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────────────\n                    Coef.   Std. Error      t  Pr(>|t|)    Lower 95%    Upper 95%\n─────────────────────────────────────────────────────────────────────────────────\n(Intercept)   2.61251      0.429442      6.08    <1e-05   1.73421     3.49082\nsales         0.000300571  0.000139295   2.16    0.0394   1.56805e-5  0.000585462\nsales ^ 2    -6.94594e-9   3.72614e-9   -1.86    0.0725  -1.45667e-8  6.7487e-10\n─────────────────────────────────────────────────────────────────────────────────","category":"section"},{"location":"examples/#Probit-regression","page":"Examples","title":"Probit regression","text":"julia> data = DataFrame(X=[1,2,2], Y=[1,0,1])\n3×2 DataFrame\n Row │ X      Y\n     │ Int64  Int64\n─────┼──────────────\n   1 │     1      1\n   2 │     2      0\n   3 │     2      1\n\njulia> probit = glm(@formula(Y ~ X), data, Binomial(), ProbitLink())\nGeneralizedLinearModel\n\nY ~ 1 + X\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────\n                Coef.  Std. Error      z  Pr(>|z|)  Lower 95%  Upper 95%\n────────────────────────────────────────────────────────────────────────\n(Intercept)   9.63839     293.909   0.03    0.9738   -566.414    585.69\nX            -4.81919     146.957  -0.03    0.9738   -292.849    283.211\n────────────────────────────────────────────────────────────────────────","category":"section"},{"location":"examples/#Negative-binomial-regression","page":"Examples","title":"Negative binomial regression","text":"julia> using GLM, RDatasets\n\njulia> quine = dataset(\"MASS\", \"quine\")\n146×5 DataFrame\n Row │ Eth   Sex   Age   Lrn   Days\n     │ Cat…  Cat…  Cat…  Cat…  Int32\n─────┼───────────────────────────────\n   1 │ A     M     F0    SL        2\n   2 │ A     M     F0    SL       11\n   3 │ A     M     F0    SL       14\n   4 │ A     M     F0    AL        5\n   5 │ A     M     F0    AL        5\n   6 │ A     M     F0    AL       13\n   7 │ A     M     F0    AL       20\n   8 │ A     M     F0    AL       22\n  ⋮  │  ⋮     ⋮     ⋮     ⋮      ⋮\n 140 │ N     F     F3    AL        3\n 141 │ N     F     F3    AL        3\n 142 │ N     F     F3    AL        5\n 143 │ N     F     F3    AL       15\n 144 │ N     F     F3    AL       18\n 145 │ N     F     F3    AL       22\n 146 │ N     F     F3    AL       37\n                     131 rows omitted\n\njulia> nbrmodel = glm(@formula(Days ~ Eth+Sex+Age+Lrn), quine, NegativeBinomial(2.0), LogLink())\nGeneralizedLinearModel\n\nDays ~ 1 + Eth + Sex + Age + Lrn\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      z  Pr(>|z|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)   2.88645      0.227144  12.71    <1e-36   2.44125     3.33164\nEth: N       -0.567515     0.152449  -3.72    0.0002  -0.86631    -0.26872\nSex: M        0.0870771    0.159025   0.55    0.5840  -0.224606    0.398761\nAge: F1      -0.445076     0.239087  -1.86    0.0627  -0.913678    0.0235251\nAge: F2       0.0927999    0.234502   0.40    0.6923  -0.366816    0.552416\nAge: F3       0.359485     0.246586   1.46    0.1449  -0.123814    0.842784\nLrn: SL       0.296768     0.185934   1.60    0.1105  -0.0676559   0.661191\n────────────────────────────────────────────────────────────────────────────\n\njulia> nbrmodel = negbin(@formula(Days ~ Eth+Sex+Age+Lrn), quine, LogLink())\nGeneralizedLinearModel\n\nDays ~ 1 + Eth + Sex + Age + Lrn\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      z  Pr(>|z|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)   2.89453      0.227415  12.73    <1e-36   2.4488      3.34025\nEth: N       -0.569341     0.152656  -3.73    0.0002  -0.868541   -0.270141\nSex: M        0.0823881    0.159209   0.52    0.6048  -0.229655    0.394431\nAge: F1      -0.448464     0.238687  -1.88    0.0603  -0.916281    0.0193536\nAge: F2       0.0880506    0.235149   0.37    0.7081  -0.372834    0.548935\nAge: F3       0.356955     0.247228   1.44    0.1488  -0.127602    0.841513\nLrn: SL       0.292138     0.18565    1.57    0.1156  -0.0717297   0.656006\n────────────────────────────────────────────────────────────────────────────\n\njulia> println(\"Estimated theta = \", round(nbrmodel.rr.d.r, digits=5))\nEstimated theta = 1.27489\n","category":"section"},{"location":"examples/#Julia-and-R-comparisons","page":"Examples","title":"Julia and R comparisons","text":"An example of a simple linear model in R is\n\n> coef(summary(lm(optden ~ carb, Formaldehyde)))\n               Estimate  Std. Error    t value     Pr(>|t|)\n(Intercept) 0.005085714 0.007833679  0.6492115 5.515953e-01\ncarb        0.876285714 0.013534536 64.7444207 3.409192e-07\n\nThe corresponding model with the GLM package is\n\njulia> using GLM, RDatasets\n\njulia> form = dataset(\"datasets\", \"Formaldehyde\")\n6×2 DataFrame\n Row │ Carb     OptDen\n     │ Float64  Float64\n─────┼──────────────────\n   1 │     0.1    0.086\n   2 │     0.3    0.269\n   3 │     0.5    0.446\n   4 │     0.6    0.538\n   5 │     0.7    0.626\n   6 │     0.9    0.782\n\njulia> lm1 = fit(LinearModel, @formula(OptDen ~ Carb), form)\nLinearModel\n\nOptDen ~ 1 + Carb\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)  0.00508571  0.00783368   0.65    0.5516  -0.0166641  0.0268355\nCarb         0.876286    0.0135345   64.74    <1e-06   0.838708   0.913864\n───────────────────────────────────────────────────────────────────────────\n\nA more complex example in R is\n\n> coef(summary(lm(sr ~ pop15 + pop75 + dpi + ddpi, LifeCycleSavings)))\n                 Estimate   Std. Error    t value     Pr(>|t|)\n(Intercept) 28.5660865407 7.3545161062  3.8841558 0.0003338249\npop15       -0.4611931471 0.1446422248 -3.1885098 0.0026030189\npop75       -1.6914976767 1.0835989307 -1.5609998 0.1255297940\ndpi         -0.0003369019 0.0009311072 -0.3618293 0.7191731554\nddpi         0.4096949279 0.1961971276  2.0881801 0.0424711387\n\nwith the corresponding Julia code\n\njulia> LifeCycleSavings = dataset(\"datasets\", \"LifeCycleSavings\")\n50×6 DataFrame\n Row │ Country         SR       Pop15    Pop75    DPI      DDPI\n     │ String15        Float64  Float64  Float64  Float64  Float64\n─────┼─────────────────────────────────────────────────────────────\n   1 │ Australia         11.43    29.35     2.87  2329.68     2.87\n   2 │ Austria           12.07    23.32     4.41  1507.99     3.93\n   3 │ Belgium           13.17    23.8      4.43  2108.47     3.82\n   4 │ Bolivia            5.75    41.89     1.67   189.13     0.22\n   5 │ Brazil            12.88    42.19     0.83   728.47     4.56\n   6 │ Canada             8.79    31.72     2.85  2982.88     2.43\n   7 │ Chile              0.6     39.74     1.34   662.86     2.67\n   8 │ China             11.9     44.75     0.67   289.52     6.51\n  ⋮  │       ⋮            ⋮        ⋮        ⋮        ⋮        ⋮\n  44 │ United States      7.56    29.81     3.43  4001.89     2.45\n  45 │ Venezuela          9.22    46.4      0.9    813.39     0.53\n  46 │ Zambia            18.56    45.25     0.56   138.33     5.14\n  47 │ Jamaica            7.72    41.12     1.73   380.47    10.23\n  48 │ Uruguay            9.24    28.13     2.72   766.54     1.88\n  49 │ Libya              8.89    43.69     2.07   123.58    16.71\n  50 │ Malaysia           4.71    47.2      0.66   242.69     5.08\n                                                    35 rows omitted\n\njulia> fm2 = fit(LinearModel, @formula(SR ~ Pop15 + Pop75 + DPI + DDPI), LifeCycleSavings)\nLinearModel\n\nSR ~ 1 + Pop15 + Pop75 + DPI + DDPI\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────────────\n                    Coef.   Std. Error      t  Pr(>|t|)    Lower 95%    Upper 95%\n─────────────────────────────────────────────────────────────────────────────────\n(Intercept)  28.5661       7.35452       3.88    0.0003  13.7533      43.3788\nPop15        -0.461193     0.144642     -3.19    0.0026  -0.752518    -0.169869\nPop75        -1.6915       1.0836       -1.56    0.1255  -3.87398      0.490983\nDPI          -0.000336902  0.000931107  -0.36    0.7192  -0.00221225   0.00153844\nDDPI          0.409695     0.196197      2.09    0.0425   0.0145336    0.804856\n─────────────────────────────────────────────────────────────────────────────────\n\nThe glm function (or equivalently, fit(GeneralizedLinearModel, ...)) works similarly to the R glm function except that the family argument is replaced by a Distribution type and, optionally, a Link type. The first example from ?glm in R is\n\nglm> ## Dobson (1990) Page 93: Randomized Controlled Trial : (slightly modified)\nglm> counts <- c(18,17,15,20,10,21,25,13,13)\n\nglm> outcome <- gl(3,1,9)\n\nglm> treatment <- gl(3,3)\n\nglm> print(d.AD <- data.frame(treatment, outcome, counts))\n  treatment outcome counts\n1         1       1     18\n2         1       2     17\n3         1       3     15\n4         2       1     20\n5         2       2     10\n6         2       3     21\n7         3       1     25\n8         3       2     13\n9         3       3     13\n\nglm> glm.D93 <- glm(counts ~ outcome + treatment, family=poisson())\n\nglm> anova(glm.D93)\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: counts\n\nTerms added sequentially (first to last)\n\n\n          Df Deviance Resid. Df Resid. Dev\nNULL                          8    10.3928\noutcome    2   5.2622         6     5.1307\ntreatment  2   0.0132         4     5.1175\n\nglm> ## No test:\nglm> summary(glm.D93)\n\nCall:\nglm(formula = counts ~ outcome + treatment, family = poisson())\n\nDeviance Residuals:\n      1        2        3        4        5        6        7        8        9\n-0.6122   1.0131  -0.2819  -0.2498  -0.9784   1.0777   0.8162  -0.1155  -0.8811\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)   3.0313     0.1712  17.711   <2e-16 ***\noutcome2     -0.4543     0.2022  -2.247   0.0246 *\noutcome3     -0.2513     0.1905  -1.319   0.1870\ntreatment2    0.0198     0.1990   0.100   0.9207\ntreatment3    0.0198     0.1990   0.100   0.9207\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 10.3928  on 8  degrees of freedom\nResidual deviance:  5.1175  on 4  degrees of freedom\nAIC: 56.877\n\nNumber of Fisher Scoring iterations: 4\n\nIn Julia this becomes\n\njulia> using DataFrames, CategoricalArrays, GLM\n\njulia> dobson = DataFrame(Counts    = [18.,17,15,20,10,21,25,13,13],\n                          Outcome   = categorical([1,2,3,1,2,3,1,2,3]),\n                          Treatment = categorical([1,1,1,2,2,2,3,3,3]))\n9×3 DataFrame\n Row │ Counts   Outcome  Treatment\n     │ Float64  Cat…     Cat…\n─────┼─────────────────────────────\n   1 │    18.0  1        1\n   2 │    17.0  2        1\n   3 │    15.0  3        1\n   4 │    20.0  1        2\n   5 │    10.0  2        2\n   6 │    21.0  3        2\n   7 │    25.0  1        3\n   8 │    13.0  2        3\n   9 │    13.0  3        3\n\njulia> gm1 = fit(GeneralizedLinearModel, @formula(Counts ~ Outcome + Treatment), dobson, Poisson())\nGeneralizedLinearModel\n\nCounts ~ 1 + Outcome + Treatment\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────────\n                   Coef.  Std. Error      z  Pr(>|z|)  Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)    3.03128      0.171155  17.71    <1e-69   2.69582    3.36674\nOutcome: 2    -0.454255     0.202171  -2.25    0.0246  -0.850503  -0.0580079\nOutcome: 3    -0.251314     0.190476  -1.32    0.1870  -0.624641   0.122012\nTreatment: 2   0.0198026    0.199017   0.10    0.9207  -0.370264   0.409869\nTreatment: 3   0.0198026    0.199017   0.10    0.9207  -0.370264   0.409869\n────────────────────────────────────────────────────────────────────────────\n\njulia> round(deviance(gm1), digits=5)\n5.11746","category":"section"},{"location":"examples/#Linear-regression-with-PowerLink","page":"Examples","title":"Linear regression with PowerLink","text":"In this example, we choose the best model from a set of λs, based on minimum BIC.\n\njulia> using GLM, RDatasets, StatsBase, DataFrames, Optim\n\njulia> trees = DataFrame(dataset(\"datasets\", \"trees\"))\n31×3 DataFrame\n Row │ Girth    Height  Volume  \n     │ Float64  Int64   Float64 \n─────┼──────────────────────────\n   1 │     8.3      70     10.3\n   2 │     8.6      65     10.3\n   3 │     8.8      63     10.2\n   4 │    10.5      72     16.4\n   5 │    10.7      81     18.8\n   6 │    10.8      83     19.7\n   7 │    11.0      66     15.6\n   8 │    11.0      75     18.2\n  ⋮  │    ⋮       ⋮        ⋮\n  25 │    16.3      77     42.6\n  26 │    17.3      81     55.4\n  27 │    17.5      82     55.7\n  28 │    17.9      80     58.3\n  29 │    18.0      80     51.5\n  30 │    18.0      80     51.0\n  31 │    20.6      87     77.0\n                 16 rows omitted\n                 \njulia> bic_glm(λ) = bic(glm(@formula(Volume ~ Height + Girth), trees, Normal(), PowerLink(λ)));\n\njulia> optimal_bic = optimize(bic_glm, -1.0, 1.0);\n\njulia> round(optimal_bic.minimizer, digits = 5) # Optimal λ\n0.40935\n\njulia> glm(@formula(Volume ~ Height + Girth), trees, Normal(), PowerLink(optimal_bic.minimizer)) # Best model\nGeneralizedLinearModel\n\nVolume ~ 1 + Height + Girth\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      z  Pr(>|z|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)  -1.07586    0.352543    -3.05    0.0023  -1.76684    -0.384892\nHeight        0.0232172  0.00523331   4.44    <1e-05   0.0129601   0.0334743\nGirth         0.242837   0.00922555  26.32    <1e-99   0.224756    0.260919\n────────────────────────────────────────────────────────────────────────────\n\njulia> round(optimal_bic.minimum, digits=5)\n156.37638","category":"section"},{"location":"#GLM.jl-Manual","page":"Home","title":"GLM.jl Manual","text":"Linear and generalized linear models in Julia","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"Pkg.add(\"GLM\")\n\nwill install this package and its dependencies, which includes the Distributions package.\n\nThe RDatasets package is useful for fitting models on standard R datasets to compare the results with those from R.","category":"section"},{"location":"#Fitting-GLM-models","page":"Home","title":"Fitting GLM models","text":"Two methods can be used to fit a Generalized Linear Model (GLM): glm(formula, data, family, link) and glm(X, y, family, link). Their arguments must be:\n\nformula: a StatsModels.jl Formula object referring to columns in data; for example, if column names are :Y, :X1, and :X2, then a valid formula is @formula(Y ~ X1 + X2)\ndata: a table in the Tables.jl definition, e.g. a data frame; rows with missing values are ignored\nX a matrix holding values of the independent variable(s) in columns\ny a vector holding values of the dependent variable (including if appropriate the intercept)\nfamily: chosen from Bernoulli(), Binomial(), Gamma(), Geometric(), Normal(), Poisson(), or NegativeBinomial(θ)\nlink: chosen from the list below, for example, LogitLink() is a valid link for the Binomial() family\n\nTypical distributions for use with glm and their canonical link functions are\n\n       Bernoulli (LogitLink)\n        Binomial (LogitLink)\n           Gamma (InverseLink)\n       Geometric (LogLink)\n InverseGaussian (InverseSquareLink)\nNegativeBinomial (NegativeBinomialLink, often used with LogLink)\n          Normal (IdentityLink)\n         Poisson (LogLink)\n\nCurrently the available Link types are\n\nCauchitLink\nCloglogLink\nIdentityLink\nInverseLink\nInverseSquareLink\nLogitLink\nLogLink\nNegativeBinomialLink\nPowerLink\nProbitLink\nSqrtLink\n\nNote that the canonical link for negative binomial regression is NegativeBinomialLink, but in practice one typically uses LogLink. The NegativeBinomial distribution belongs to the exponential family only if θ (the shape parameter) is fixed, thus θ has to be provided if we use glm with NegativeBinomial family. If one would like to also estimate θ, then negbin(formula, data, link) should be used instead.\n\nAn intercept is included in any GLM by default.","category":"section"},{"location":"#Categorical-variables","page":"Home","title":"Categorical variables","text":"Categorical variables will be dummy coded by default if they are non-numeric or if they are CategoricalVectors within a Tables.jl table (DataFrame, JuliaDB table, named tuple of vectors, etc). Alternatively, you can pass an explicit contrasts argument if you would like a different contrast coding system or if you are not using DataFrames.\n\nThe response (dependent) variable may not be categorical.\n\nUsing a CategoricalVector constructed with categorical or categorical!:\n\njulia> using CategoricalArrays, DataFrames, GLM, StableRNGs\n\njulia> rng = StableRNG(1); # Ensure example can be reproduced\n\njulia> data = DataFrame(y = rand(rng, 100), x = categorical(repeat([1, 2, 3, 4], 25)));\n\n\njulia> lm(@formula(y ~ x), data)\nLinearModel\n\ny ~ 1 + x\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)   0.490985    0.0564176   8.70    <1e-13   0.378997    0.602973\nx: 2          0.0527655   0.0797865   0.66    0.5100  -0.105609    0.21114\nx: 3          0.0955446   0.0797865   1.20    0.2341  -0.0628303   0.25392\nx: 4         -0.032673    0.0797865  -0.41    0.6831  -0.191048    0.125702\n───────────────────────────────────────────────────────────────────────────\n\nUsing contrasts:\n\njulia> using StableRNGs\n\njulia> data = DataFrame(y = rand(StableRNG(1), 100), x = repeat([1, 2, 3, 4], 25));\n\njulia> lm(@formula(y ~ x), data, contrasts = Dict(:x => DummyCoding()))\nLinearModel\n\ny ~ 1 + x\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)   0.490985    0.0564176   8.70    <1e-13   0.378997    0.602973\nx: 2          0.0527655   0.0797865   0.66    0.5100  -0.105609    0.21114\nx: 3          0.0955446   0.0797865   1.20    0.2341  -0.0628303   0.25392\nx: 4         -0.032673    0.0797865  -0.41    0.6831  -0.191048    0.125702\n───────────────────────────────────────────────────────────────────────────","category":"section"},{"location":"#Comparing-models-with-F-test","page":"Home","title":"Comparing models with F-test","text":"Comparisons between two or more linear models can be performed using the ftest function, which computes an F-test between each pair of subsequent models and reports fit statistics:\n\njulia> using DataFrames, GLM, StableRNGs\n\njulia> data = DataFrame(y = (1:50).^2 .+ randn(StableRNG(1), 50), x = 1:50);\n\njulia> ols_lin = lm(@formula(y ~ x), data);\n\njulia> ols_sq = lm(@formula(y ~ x + x^2), data);\n\njulia> ftest(ols_lin, ols_sq)\nF-test: 2 models fitted on 50 observations\n─────────────────────────────────────────────────────────────────────────────────\n     DOF  ΔDOF           SSR           ΔSSR      R²     ΔR²            F*   p(>F)\n─────────────────────────────────────────────────────────────────────────────────\n[1]    3        1731979.2266                 0.9399\n[2]    4     1       40.7581  -1731938.4685  1.0000  0.0601  1997177.0357  <1e-99\n─────────────────────────────────────────────────────────────────────────────────","category":"section"},{"location":"#Methods-applied-to-fitted-models","page":"Home","title":"Methods applied to fitted models","text":"Many of the methods provided by this package have names similar to those in R.\n\nadjr2: adjusted R² for a linear model (an alias for adjr²)\naic: Akaike's Information Criterion\naicc: corrected Akaike's Information Criterion for small sample sizes\nbic: Bayesian Information Criterion\ncoef: estimates of the coefficients in the model\nconfint: confidence intervals for coefficients\ncooksdistance: Cook's distance for each observation\ndeviance: measure of the model fit, weighted residual sum of squares for lm's\ndispersion: dispersion (or scale) parameter for a model's distribution\ndof: number of degrees of freedom consumed in the model\ndof_residual: degrees of freedom for residuals, when meaningful\nfitted: fitted values of the model\nglm: fit a generalized linear model (an alias for fit(GeneralizedLinearModel, ...))\nlm: fit a linear model (an alias for fit(LinearModel, ...))\nloglikelihood: log-likelihood of the model\nmodelmatrix: design matrix\nnobs: number of rows, or sum of the weights when prior weights are specified\nnulldeviance: deviance of the model with all predictors removed\nnullloglikelihood: log-likelihood of the model with all predictors removed\npredict: predicted values of the dependent variable from the fitted model\nr2: R² of a linear model (an alias for r²)\nresiduals: vector of residuals from the fitted model\nresponse: model response (a.k.a the dependent variable)\nstderror: standard errors of the coefficients\nvcov: variance-covariance matrix of the coefficient estimates\n\nNote that the canonical link for negative binomial regression is NegativeBinomialLink, but in practice one typically uses LogLink.\n\njulia> using GLM, DataFrames, StatsBase\n\njulia> data = DataFrame(X=[1,2,3], y=[2,4,7]);\n\njulia> mdl = lm(@formula(y ~ X), data);\n\njulia> round.(coef(mdl); digits=8)\n2-element Vector{Float64}:\n -0.66666667\n  2.5\n\njulia> round(r2(mdl); digits=8)\n0.98684211\n\njulia> round(aic(mdl); digits=8)\n5.84251593\n\nThe predict method returns predicted values of response variable from covariate values in an input newX. If newX is omitted then the fitted response values from the model are returned.\n\njulia> test_data = DataFrame(X=[4]);\n\njulia> round.(predict(mdl, test_data); digits=8)\n1-element Vector{Float64}:\n 9.33333333\n\nThe cooksdistance method computes Cook's distance for each observation used to fit a linear model, giving an estimate of the influence of each data point. Note that it's currently only implemented for linear models without weights.\n\njulia> round.(cooksdistance(mdl); digits=8)\n3-element Vector{Float64}:\n 2.5\n 0.25\n 2.5","category":"section"},{"location":"#Separation-of-response-object-and-predictor-object","page":"Home","title":"Separation of response object and predictor object","text":"The general approach in this code is to separate functionality related to the response from that related to the linear predictor.  This allows for greater generality by mixing and matching different subtypes of the abstract type LinPred and the abstract type ModResp.\n\nA LinPred type incorporates the parameter vector and the model matrix.  The parameter vector is a dense numeric vector but the model matrix can be dense or sparse.  A LinPred type must incorporate some form of a decomposition of the weighted model matrix that allows for the solution of a system X'W * X * delta=X'wres where W is a diagonal matrix of \"X weights\", provided as a vector of the square roots of the diagonal elements, and wres is a weighted residual vector.\n\nCurrently there are two dense predictor types, DensePredQR and DensePredChol, and the usual caveats apply.  The Cholesky version is faster but somewhat less accurate than that QR version. The skeleton of a distributed predictor type is in the code but not yet fully fleshed out.  Because Julia by default uses OpenBLAS, which is already multi-threaded on multicore machines, there may not be much advantage in using distributed predictor types.\n\nA ModResp type must provide methods for the wtres and sqrtxwts generics.  Their values are the arguments to the updatebeta methods of the LinPred types.  The Float64 value returned by updatedelta is the value of the convergence criterion.\n\nSimilarly, LinPred types must provide a method for the linpred generic.  In general linpred takes an instance of a LinPred type and a step factor.  Methods that take only an instance of a LinPred type use a default step factor of 1.  The value of linpred is the argument to the updatemu method for ModResp types.  The updatemu method returns the updated deviance.","category":"section"},{"location":"#Debugging-failed-fits","page":"Home","title":"Debugging failed fits","text":"In the rare cases when a fit of a generalized linear model fails, it can be useful to enable more output from the fitting steps. This can be done through the Julia logging mechanism by setting ENV[\"JULIA_DEBUG\"] = GLM. Enabling debug output will result in ouput like the following\n\n┌ Debug: Iteration: 1, deviance: 5.129147109764238, diff.dev.:0.05057195315968688\n└ @ GLM ~/.julia/dev/GLM/src/glmfit.jl:418\n┌ Debug: Iteration: 2, deviance: 5.129141077001254, diff.dev.:6.032762984276019e-6\n└ @ GLM ~/.julia/dev/GLM/src/glmfit.jl:418\n┌ Debug: Iteration: 3, deviance: 5.129141077001143, diff.dev.:1.1102230246251565e-13\n└ @ GLM ~/.julia/dev/GLM/src/glmfit.jl:418","category":"section"}]
}
